{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e5cb309-aeba-4e8d-a88c-9cd6bb68714f",
   "metadata": {},
   "source": [
    "## Visualize All Deep Pretrained Classification model-metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bdc28bb-f486-48d4-9606-f91a153f7f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e83a044b-6701-4093-9e89-4c8965207116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from torch.optim import Adam\n",
    "from transformers import AutoTokenizer, GPT2Tokenizer \n",
    "from transformers import GPT2Model, BartForSequenceClassification,\\\n",
    "                         RobertaForSequenceClassification,\\\n",
    "                         DistilBertForSequenceClassification \n",
    "\n",
    "from model_pipeline import *\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay\n",
    "\n",
    "\n",
    "import gc\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad74a1-da9c-4952-9248-e413f528f935",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f38ff6-6ca5-487c-bf3e-267d82b7aa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0d3c0d1-d27b-4976-b22f-29b58f7da6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>medium</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tsietsi Mashinini speaks on Soweto uprising</td>\n",
       "      <td>Image: Tsietsi Mashinini \\nBy BO Staff Writer\\...</td>\n",
       "      <td>https://blackopinion.co.za/category/intnews/fe...</td>\n",
       "      <td>blackopinion</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Parliament's tourism committee to question Tot...</td>\n",
       "      <td>The National Assembly's Portfolio Committee on...</td>\n",
       "      <td>https://www.news24.com/news24/politics/parliam...</td>\n",
       "      <td>news24</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VIDEO: ANC Are Punch Drunk on Emergency Powers...</td>\n",
       "      <td>Local KDM traffic officers in KZN are punch dr...</td>\n",
       "      <td>https://sa-news.com/video-anc-are-punch-drunk-...</td>\n",
       "      <td>sa-news</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is why Mpumalanga is repurposing some TB ...</td>\n",
       "      <td>The Mpumalanga health department plans to repu...</td>\n",
       "      <td>https://www.news24.com/news24/health/spotlight...</td>\n",
       "      <td>news24</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ramaphosa Cares More For Vicious Farm Attacker...</td>\n",
       "      <td>Ramaphosa told Bloomberg News in New York in 2...</td>\n",
       "      <td>https://sa-news.com/ramaphosa-cares-more-for-v...</td>\n",
       "      <td>sa-news</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0        Tsietsi Mashinini speaks on Soweto uprising   \n",
       "1  Parliament's tourism committee to question Tot...   \n",
       "2  VIDEO: ANC Are Punch Drunk on Emergency Powers...   \n",
       "3  This is why Mpumalanga is repurposing some TB ...   \n",
       "4  Ramaphosa Cares More For Vicious Farm Attacker...   \n",
       "\n",
       "                                                text  \\\n",
       "0  Image: Tsietsi Mashinini \\nBy BO Staff Writer\\...   \n",
       "1  The National Assembly's Portfolio Committee on...   \n",
       "2  Local KDM traffic officers in KZN are punch dr...   \n",
       "3  The Mpumalanga health department plans to repu...   \n",
       "4  Ramaphosa told Bloomberg News in New York in 2...   \n",
       "\n",
       "                                                 url        medium label  \n",
       "0  https://blackopinion.co.za/category/intnews/fe...  blackopinion  fake  \n",
       "1  https://www.news24.com/news24/politics/parliam...        news24  real  \n",
       "2  https://sa-news.com/video-anc-are-punch-drunk-...       sa-news  fake  \n",
       "3  https://www.news24.com/news24/health/spotlight...        news24  real  \n",
       "4  https://sa-news.com/ramaphosa-cares-more-for-v...       sa-news  fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../../Sample_Data/For_Modeling/sarealfakenews_dataset/Combined_Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d286f21c-607c-4ebc-92b8-bbca724c32f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fake', 'real'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062016c8-6f3a-403c-bc4d-f17eea79c191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of News Labels')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "sns.countplot(x = df['label'], palette = 'Set1', alpha = 0.8)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of News Labels',fontsize=20, fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e7d008-411d-4d1a-8805-d58229ce59c5",
   "metadata": {},
   "source": [
    "## Split training-test dataset\n",
    "* One more thing to do before we start with models. We need to split train, validation and test data as separate dataframes. Numpy's split function can do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8908d627-1f6a-474e-8eb1-b4f208c8b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1564 196 196\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(112)\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=35),\n",
    "                                     [int(0.8*len(df)), int(0.9*len(df))])\n",
    "\n",
    "print(len(df_train), len(df_val), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ef9c4-c0c9-447a-947f-42e65d3b3101",
   "metadata": {},
   "source": [
    "## Preprocessing data (text tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48b368c9-4484-4a19-8bbb-b9389690424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"real\": 0,\"fake\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca963b5c-e9b6-44f6-960c-119800fa331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3df4f32-8e4b-40e5-aac5-b38389c9111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9abf15f2-4854-4f8c-a94b-d27a63fe314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.padding_side = \"left\"\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325a139e-d44c-4c10-9ae7-732fd60bfcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cb2292-c6e6-449c-8bae-5050ff065dd1",
   "metadata": {},
   "source": [
    "# Model building\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1abb0bd1-775d-418d-9880-3a9f604ce07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "LR = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e169726b-ed6f-424d-a268-0af3b3858d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aed871a-d24a-41a5-afb4-478f9181bf6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f241f3d4-936d-400b-a3a8-d0764edd490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = SimpleGPT2SequenceClassifier(hidden_size=768, num_classes=2, max_seq_len=512, gpt_model_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88c00887-73b2-46f8-8459-87de37e39d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.weight', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bart_model = BartForSequenceClassification.from_pretrained(\"facebook/bart-base\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d905d94-f785-408c-aa27-ff9f7f2b25b4",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* Now it's time to train (fine-tune) our model! Here I build a standard PyTorch training loop following [this guide](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html). <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a64a7558-e03f-49ed-aa1c-c25c3363a4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [01:16<00:00, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.124             | Train Accuracy:  0.899             | Val Loss:  0.080             | Val Accuracy:  0.939\n"
     ]
    }
   ],
   "source": [
    "train(distilbert_model, 'distilbert', df_train, df_val, labels, distilbert_tokenizer, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c84f808d-b92f-41e2-aaa3-f934f459674c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:32<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.046             | Train Accuracy:  0.966             | Val Loss:  0.096             | Val Accuracy:  0.929\n"
     ]
    }
   ],
   "source": [
    "train(roberta_model, 'roberta', df_train, df_val, labels, roberta_tokenizer, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78a6c6af-bf5d-4187-a295-d05dad2f613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/782 [00:00<04:15,  3.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/Documents/Projects/Text </span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Analysis/text_analysis/Code/Modeling/safakenews/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model_pipeline.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">69</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 66 │   │   │   </span>model.zero_grad()                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 67 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 68 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> model_name ==<span style=\"color: #808000; text-decoration-color: #808000\">'gpt2'</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 69 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>output = model(input_id, mask)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 70 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 71 │   │   │   │   </span>output = model(input_id, mask, labels=train_label)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 72 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/Documents/Projects/Text </span>                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">Analysis/text_analysis/Code/Modeling/safakenews/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">model_pipeline.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">185</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">182 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Args:</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">input_id: encoded inputs ids of sent.</span>                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">184 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>185 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>gpt_out, _ = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.gpt2model(input_ids=input_id, attention_mask=mask, return_dict   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">186 │   │   </span>batch_size = gpt_out.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">187 │   │   </span>linear_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fc1(gpt_out.view(batch_size,-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>))                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">188 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> linear_output                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_g</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">889</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 886 │   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 887 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 888 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 889 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = block(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 890 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 891 │   │   │   │   │   </span>layer_past=layer_past,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 892 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_g</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">389</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 386 │   </span>) -&gt; Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 387 │   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 388 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_1(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 389 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attn(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 390 │   │   │   </span>hidden_states,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 391 │   │   │   </span>layer_past=layer_past,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 392 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1194</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1191 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1192 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1193 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1194 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1195 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1196 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1197 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_g</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">330</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 327 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.reorder_and_upcast_attn:                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 328 │   │   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._upcast_and_reordered_attn(query, key, valu  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 329 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 330 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attn_output, attn_weights = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._attn(query, key, value, attention_mask, he  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 331 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 332 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._merge_heads(attn_output, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_heads, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.head_dim)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 333 │   │   </span>attn_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.c_proj(attn_output)                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_g</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">182</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_attn</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 179 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pruned_heads = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.pruned_heads.union(heads)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 180 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 181 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_attn</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, query, key, value, attention_mask=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>, head_mask=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 182 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attn_weights = torch.matmul(query, key.transpose(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, -<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>))                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 183 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 184 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scale_attn_weights:                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 185 │   │   │   </span>attn_weights = attn_weights / torch.full(                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.79</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.36</span> GiB already\n",
       "allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">91.31</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.51</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/Documents/Projects/Text \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mAnalysis/text_analysis/Code/Modeling/safakenews/\u001b[0m\u001b[1;33mmodel_pipeline.py\u001b[0m:\u001b[94m69\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 66 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel.zero_grad()                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 67 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 68 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m model_name ==\u001b[33m'\u001b[0m\u001b[33mgpt2\u001b[0m\u001b[33m'\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 69 \u001b[2m│   │   │   │   \u001b[0moutput = model(input_id, mask)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 71 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = model(input_id, mask, labels=train_label)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 72 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/Documents/Projects/Text \u001b[0m                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33mAnalysis/text_analysis/Code/Modeling/safakenews/\u001b[0m\u001b[1;33mmodel_pipeline.py\u001b[0m:\u001b[94m185\u001b[0m in \u001b[92mforward\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m182 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33mArgs:\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m183 \u001b[0m\u001b[2;33m│   │   │   │   \u001b[0m\u001b[33minput_id: encoded inputs ids of sent.\u001b[0m                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m184 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m185 \u001b[2m│   │   \u001b[0mgpt_out, _ = \u001b[96mself\u001b[0m.gpt2model(input_ids=input_id, attention_mask=mask, return_dict   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m186 \u001b[0m\u001b[2m│   │   \u001b[0mbatch_size = gpt_out.shape[\u001b[94m0\u001b[0m]                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m187 \u001b[0m\u001b[2m│   │   \u001b[0mlinear_output = \u001b[96mself\u001b[0m.fc1(gpt_out.view(batch_size,-\u001b[94m1\u001b[0m))                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m188 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m linear_output                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_g\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpt2.py\u001b[0m:\u001b[94m889\u001b[0m in \u001b[92mforward\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 886 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 887 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 888 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 889 \u001b[2m│   │   │   │   \u001b[0moutputs = block(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 890 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 891 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_past=layer_past,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 892 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_g\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpt2.py\u001b[0m:\u001b[94m389\u001b[0m in \u001b[92mforward\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 386 \u001b[0m\u001b[2m│   \u001b[0m) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 387 \u001b[0m\u001b[2m│   │   \u001b[0mresidual = hidden_states                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 388 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_1(hidden_states)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 389 \u001b[2m│   │   \u001b[0mattn_outputs = \u001b[96mself\u001b[0m.attn(                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 390 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 391 \u001b[0m\u001b[2m│   │   │   \u001b[0mlayer_past=layer_past,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 392 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1194\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1191 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1192 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1193 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1194 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1195 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1196 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_g\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpt2.py\u001b[0m:\u001b[94m330\u001b[0m in \u001b[92mforward\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 327 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.reorder_and_upcast_attn:                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 328 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._upcast_and_reordered_attn(query, key, valu  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 329 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 330 \u001b[2m│   │   │   \u001b[0mattn_output, attn_weights = \u001b[96mself\u001b[0m._attn(query, key, value, attention_mask, he  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 331 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 332 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m._merge_heads(attn_output, \u001b[96mself\u001b[0m.num_heads, \u001b[96mself\u001b[0m.head_dim)       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 333 \u001b[0m\u001b[2m│   │   \u001b[0mattn_output = \u001b[96mself\u001b[0m.c_proj(attn_output)                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/siyanda/anaconda3/envs/NLP/lib/python3.9/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_g\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mpt2.py\u001b[0m:\u001b[94m182\u001b[0m in \u001b[92m_attn\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 179 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.pruned_heads = \u001b[96mself\u001b[0m.pruned_heads.union(heads)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 180 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 181 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_attn\u001b[0m(\u001b[96mself\u001b[0m, query, key, value, attention_mask=\u001b[94mNone\u001b[0m, head_mask=\u001b[94mNone\u001b[0m):              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 182 \u001b[2m│   │   \u001b[0mattn_weights = torch.matmul(query, key.transpose(-\u001b[94m1\u001b[0m, -\u001b[94m2\u001b[0m))                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 183 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 184 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.scale_attn_weights:                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 185 \u001b[0m\u001b[2m│   │   │   \u001b[0mattn_weights = attn_weights / torch.full(                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m20.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m7.79\u001b[0m GiB total capacity; \u001b[1;36m5.36\u001b[0m GiB already\n",
       "allocated; \u001b[1;36m91.31\u001b[0m MiB free; \u001b[1;36m5.51\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(gpt2_model, 'gpt2', df_train, df_val, labels, gpt2_tokenizer, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964298fa-5768-428b-99cc-5315d25c0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(bart_model, 'bart', df_train, df_val,labels, bart_tokenizer, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48e1d8-7b15-4c26-9f30-a21e0ba652a7",
   "metadata": {},
   "source": [
    "* Looks like the model is already well trained after 1 epoch! <br>\n",
    "  This is probably due to the fact that as a pre-trained model with gigantic number of parameters, <br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d3846-aef0-4f6b-9835-3a0aafff1357",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "* After model training, it's recommended to use the test data to evaluate the model<br>\n",
    "  performance on unseen data. I build the evaluate function according this PyTorch guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cca687-d41e-4082-ac62-8a6db3d40b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels, pred_labels = evaluate(model, model_name, df_test,labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052ceec5-8956-4a5b-b35c-beb00d084a3d",
   "metadata": {},
   "source": [
    "* Another good gauge of model performance is the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626061e2-a5f8-4a90-83ef-4f040eda56b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix.\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "cm = confusion_matrix(y_true=true_labels, y_pred=pred_labels, labels=range(len(labels)), normalize='true')\n",
    "disp= ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(labels.keys()))\n",
    "ax.set_title(\"BART SA Fake News Classifier Confusion Matrix\")\n",
    "disp.plot(ax=ax)\n",
    "plt.savefig('bart.png', format=\"png\", dpi=300, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0445142a-4da0-424c-901f-440f32c16b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"BART Accuracy Score: {accuracy_score(y_true=true_labels, y_pred=pred_labels)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
